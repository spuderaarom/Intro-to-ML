import numpy as np
import pandas as pd
from google.colab import drive
import os
from matplotlib import pyplot as plt
#drive.mount('/content/drive')
# print(os.listdir(os.getcwd()+'/drive/MyDrive'))

#df = pd.read_csv(r"C:\Users\aaron\OneDrive\Desktop\UNCC\23-24\Machine Learning\Assignment 1\D3.csv")
file_path = './drive/MyDrive/D3.csv'
df = pd.DataFrame(pd.read_csv(file_path))
df.head()

# @title X1 vs Y
df.plot(kind='scatter', x='X1', y='Y', s=25, alpha=1)
# plt.gca().spines[['top', 'right',]].set_visible(False)

# @title X2 vs Y
df.plot(kind='scatter', x='X2', y='Y', s=32, alpha=.8)

# @title X3 vs Y
df.plot(kind='scatter', x='X3', y='Y', s=32, alpha=.8)

X1 = df.values[:, 0]
X2 = df.values[:, 1]
X3 = df.values[:, 2]
Y = df.values[:, 3]
m = len(Y)
n1 = len(X1)
n2 = len(X2)
n3 = len(X3)

X_0= np.ones((m,1))
X_1 = X1.reshape(m, 1)
X_2 = X2.reshape(m, 1)
X_3 = X3.reshape(m, 1)

Xtotal1 = np.hstack((X_0, X_1))
Xtotal2 = np.hstack((X_0, X_2))
Xtotal3 = np.hstack((X_0, X_3))
theta = np.zeros(2)
theta

def compute_cost( X, Y, theta):
    predictions = X.dot(theta)
    errors = np.subtract(predictions, Y)
    sqrErrors = np.square(errors)
    J = 1 / (2 * m) * np.sum(sqrErrors)
    return J

# Compute cost for theta values
cost1 = compute_cost(Xtotal1, Y, theta)
cost2 = compute_cost(Xtotal2, Y, theta)
cost3 = compute_cost(Xtotal3, Y, theta)

print('Cost 1 =', cost1)
print('Cost 2 =', cost2)
print('Cost 3 =', cost3)

def gradient_descent(X, Y, theta, alpha, iterations):
    m = len(Y)  # Number of training examples
    cost_history = np.zeros(iterations)

    for i in range(iterations):
        predictions = X.dot(theta)
        errors = np.subtract(predictions, Y)
        sum_delta = (alpha / m) * X.transpose().dot(errors)
        theta -= sum_delta
        cost_history[i] = compute_cost(X, Y, theta)

    return theta, cost_history
# @title Linear Regression of X1 vs Y
iterations = 3000
alpha = 0.01

theta, cost_history = gradient_descent(Xtotal1, Y, theta, alpha, iterations)
print('Final value of theta =', theta)
print('cost_history of X1=', cost_history)

df.plot(kind='scatter', x='X1', y='Y', s=25, alpha=1)
plt.plot(Xtotal1[:, 1], Xtotal1.dot(theta), color='green', label='Linear Regression')

plt.plot(range(1, iterations + 1), cost_history, color='blue')
plt.rcParams["figure.figsize"] = (10, 10)
plt.grid(True)

plt.xlabel('Iterations')
plt.ylabel('Cost (J)')
plt.title('Gradient Descent Convergence for X1')
plt.show()

iterations = 3000
alpha = 0.01

theta, cost_history = gradient_descent(Xtotal2, Y, theta, 0.01, 3000)
print('Theta =', theta)
print('Cost History of X2=', cost_history)

# @title Linear Regression of X2 vs Y
df.plot(kind='scatter', x='X2', y='Y', s=32, alpha=.8) #plots ppoints
plt.plot(Xtotal2[:, 1], Xtotal2.dot(theta), color='green', label='Linear Regression') #plots line

plt.plot(Xtotal2[:, 1], Xtotal2.dot(theta), color='green', label='Linear Regression') #plots line

iterations = 3000
alpha = 0.01

theta, cost_history = gradient_descent(Xtotal3, Y, theta, alpha, iterations)
print('Final value of theta =', theta)
print('cost_history of X3=', cost_history)

# @title Linear Regression X3 vs Y
df.plot(kind='scatter', x='X3', y='Y', s=32, alpha=.8)
plt.plot(Xtotal3[:, 1], Xtotal3.dot(theta), color='green', label='Linear Regression')

plt.plot(range(1, iterations + 1), cost_history, color='blue')
plt.rcParams["figure.figsize"] = (10, 6)
plt.grid(True)

plt.xlabel('Number of iterations')
plt.ylabel('Cost (J)')
plt.title('Convergence of gradient descent for X3')

# Show the plot
plt.show()

XtotalOfAll = np.hstack((X_0, X_1,X_2,X_3))
XtotalOfAll[:5]
theta = np.zeros(4)
theta

def compute_cost( X, Y, theta):
    predictions = X.dot(theta)
    errors = np.subtract(predictions, Y)
    sqrErrors = np.square(errors)
    J = 1 / (2 * m) * np.sum(sqrErrors)
    return J

iterations = 450
alpha = 0.01
costOfAll = compute_cost(XtotalOfAll, Y, theta)
print('The cost all inputs and outputs Y =', costOfAll)

def gradient_descent(X, Y, theta, alpha, iterations):
    m = len(Y)  # Number of training examples
    cost_history = np.zeros(iterations)

    for i in range(iterations):
        predictions = X.dot(theta)
        errors = np.subtract(predictions, Y)
        sum_delta = (alpha / m) * X.transpose().dot(errors)
        theta -= sum_delta
        cost_history[i] = compute_cost(X, Y, theta)

    return theta, cost_history
theta, cost_history = gradient_descent(XtotalOfAll, Y, theta, alpha, iterations)
print('Final theta cost =', theta)
print('Cost History of X total=', cost_history)
